{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np \n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集太大，用一小部分试一试\n",
    "data,label = train_data[0][0:1000],train_data[1][0:1000]\n",
    "# 考虑到灰度数据用conv2d处理的时候需要增加一个通道维度，需要对数据集进行一个变换\n",
    "data = np.expand_dims(data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 撸个简单的卷积神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,3,3,1,1)\n",
    "        self.pooling = nn.MaxPool2d(2)\n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc =  nn.Linear(14*14*3,10)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x= self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一下Batch_size\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# 构造数据集\n",
    "data = torch.Tensor(data)\n",
    "label = torch.LongTensor(label)\n",
    "# 划分数据集\n",
    "x_train,x_test,y_train,y_test = train_test_split(data,label)\n",
    "x_train = x_train[150:]\n",
    "y_train = y_train[150:]\n",
    "\n",
    "x_valid = x_train[0:150]\n",
    "y_valid = y_train[0:150]\n",
    "\n",
    "TrainData = TensorDataset(x_train,y_train)\n",
    "ValidData = TensorDataset(x_valid,y_valid)\n",
    "TestData = TensorDataset(x_test,y_test)\n",
    "\n",
    "# 搞个loader\n",
    "TrainLoader = DataLoader(TrainData,batch_size=batch_size,shuffle=True)\n",
    "ValidLoader = DataLoader(ValidData,batch_size=batch_size,shuffle=True)\n",
    "TestLoader = DataLoader(TestData,batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练(我觉得我有必要写一个Trainer的库，方便之后进行调用，但是这次主要是为了搞wandb)\n",
    "# 在写主要的训练函数之前，需要先写一个eval函数，用于对验证集和测试集进行评估。\n",
    "# 再补一个函数，对于分类的数据，计算accuracy,输入为np.array类型的输出值（因此需要在训练过程中进行一个数据转换）\n",
    "def compute_acc(pred,label):\n",
    "    return np.equal(pred,label).mean()\n",
    "    \n",
    "def eval(model,dataloader,loss_function,device):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    model.eval() # 禁用dropout和bn层\n",
    "    for idx,(data,label) in tqdm(enumerate(dataloader)):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(data)\n",
    "        # 计算loss值\n",
    "        loss += loss_function(out,label).item()\n",
    "        # 计算accuracy\n",
    "        # argmax这块先留个心眼，我不确定这个axis是不是需要微调\n",
    "        pred = torch.argmax(out,axis=1)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        acc += compute_acc(pred,label)\n",
    "    \n",
    "    avg_loss = loss/ len(dataloader)\n",
    "    avg_acc = acc / len(dataloader)\n",
    "    return avg_loss,avg_acc\n",
    "         \n",
    "# 现在可以写train函数了   (model需要先放到GPU上才可以)\n",
    "def train(model,optimizer,loss_function,epoch,trainloader,validloader,testloader,device):\n",
    "    # 开始训练\n",
    "    for i in tqdm(range(1,epoch+1)):\n",
    "        # 进行一个model.train()\n",
    "        model.train()\n",
    "        # 对于训练过程中每一个epoch的参数记录\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for idx,(data,label) in tqdm(enumerate(trainloader)):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            out = model(data)\n",
    "            loss = loss_function(out,label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 计算出来损失函数\n",
    "            train_loss += loss.item()\n",
    "            # 转到cpu上计算准确率\n",
    "            pred = torch.argmax(out,axis=1)\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            train_acc += compute_acc(pred,label)\n",
    "        # 一个epoch结束后，计算均值\n",
    "        train_loss = train_loss / len(trainloader)\n",
    "        train_acc = train_acc  / len(trainloader)\n",
    "        # 剩下的是验证集和测试集\n",
    "        valid_loss,valid_acc = eval(model,validloader,loss_function,device=device)\n",
    "        test_loss,test_acc = eval(model,testloader,loss_function,device=device)            \n",
    "        print('epoch:',i)\n",
    "        print('Train loss',train_loss,'train_acc',train_acc)\n",
    "        print('Valid loss',valid_loss,'valid_acc',valid_acc)\n",
    "        print('Test loss',test_loss,'test acc',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用cuda,同时设置一些参数\n",
    "device = torch.device(0)\n",
    "epoch = 20\n",
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 169.59it/s]0:00<?, ?it/s]\n",
      "5it [00:00, 555.67it/s]\n",
      "8it [00:00, 380.98it/s]\n",
      "  5%|▌         | 1/20 [00:00<00:02,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Train loss 2.09795417911128 train_acc 0.31359649122807015\n",
      "Valid loss 1.7310303926467896 valid_acc 0.6056818181818182\n",
      "Test loss 1.778806820511818 test acc 0.5108173076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 169.42it/s]\n",
      "5it [00:00, 525.56it/s]\n",
      "8it [00:00, 615.50it/s]\n",
      " 10%|█         | 2/20 [00:00<00:02,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Train loss 1.477916817916067 train_acc 0.7050438596491229\n",
      "Valid loss 1.1660857439041137 valid_acc 0.7863636363636364\n",
      "Test loss 1.2636752426624298 test acc 0.7205528846153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 237.07it/s]\n",
      "5it [00:00, 714.39it/s]\n",
      "8it [00:00, 800.08it/s]\n",
      " 15%|█▌        | 3/20 [00:00<00:02,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Train loss 0.9774685439310575 train_acc 0.8004385964912281\n",
      "Valid loss 0.7643369913101197 valid_acc 0.884659090909091\n",
      "Test loss 0.9172647669911385 test acc 0.7950721153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 249.75it/s]\n",
      "5it [00:00, 623.54it/s]\n",
      "8it [00:00, 724.81it/s]\n",
      " 20%|██        | 4/20 [00:00<00:01,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Train loss 0.6924863692961241 train_acc 0.8601973684210527\n",
      "Valid loss 0.5671532034873963 valid_acc 0.9045454545454545\n",
      "Test loss 0.7444128766655922 test acc 0.8179086538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 254.77it/s]\n",
      "5it [00:00, 625.18it/s]\n",
      "8it [00:00, 799.89it/s]\n",
      " 25%|██▌       | 5/20 [00:00<00:01,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Train loss 0.5305022173806241 train_acc 0.8700657894736842\n",
      "Valid loss 0.42557480931282043 valid_acc 0.934659090909091\n",
      "Test loss 0.6368088200688362 test acc 0.8575721153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 273.14it/s]\n",
      "5it [00:00, 830.52it/s]\n",
      "8it [00:00, 798.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Train loss 0.4247501821894395 train_acc 0.9002192982456141\n",
      "Valid loss 0.35903558135032654 valid_acc 0.934659090909091\n",
      "Test loss 0.581682376563549 test acc 0.8587740384615384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 239.83it/s]\n",
      "5it [00:00, 997.55it/s]\n",
      "8it [00:00, 614.43it/s]\n",
      " 35%|███▌      | 7/20 [00:00<00:01,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Train loss 0.35156616803846863 train_acc 0.9155701754385964\n",
      "Valid loss 0.3020439982414246 valid_acc 0.947159090909091\n",
      "Test loss 0.5383906736969948 test acc 0.8780048076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 285.12it/s]\n",
      "5it [00:00, 831.02it/s]\n",
      "8it [00:00, 800.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Train loss 0.30090986898070887 train_acc 0.9331140350877194\n",
      "Valid loss 0.24809558391571046 valid_acc 0.96875\n",
      "Test loss 0.5097426995635033 test acc 0.8740985576923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 296.85it/s]\n",
      "5it [00:00, 625.03it/s]\n",
      "8it [00:00, 727.22it/s]\n",
      " 45%|████▌     | 9/20 [00:01<00:01,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "Train loss 0.2585523316734715 train_acc 0.9380482456140351\n",
      "Valid loss 0.2167094886302948 valid_acc 0.953409090909091\n",
      "Test loss 0.5063803605735302 test acc 0.8722956730769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 271.32it/s]\n",
      "5it [00:00, 800.78it/s]\n",
      "8it [00:00, 1142.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "Train loss 0.2236514577740117 train_acc 0.9583333333333333\n",
      "Valid loss 0.18981288075447084 valid_acc 0.978409090909091\n",
      "Test loss 0.4906037747859955 test acc 0.8677884615384616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 311.47it/s]\n",
      "5it [00:00, 832.24it/s]\n",
      "8it [00:00, 704.88it/s]\n",
      " 55%|█████▌    | 11/20 [00:01<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11\n",
      "Train loss 0.1913856805155152 train_acc 0.96875\n",
      "Valid loss 0.15125632286071777 valid_acc 0.99375\n",
      "Test loss 0.46745049208402634 test acc 0.8897235576923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 279.34it/s]\n",
      "5it [00:00, 999.98it/s]\n",
      "8it [00:00, 997.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12\n",
      "Train loss 0.16732415831402728 train_acc 0.975328947368421\n",
      "Valid loss 0.1345185309648514 valid_acc 0.99375\n",
      "Test loss 0.4606877751648426 test acc 0.8888221153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 254.98it/s]\n",
      "5it [00:00, 416.45it/s]\n",
      "8it [00:00, 694.62it/s]\n",
      " 65%|██████▌   | 13/20 [00:01<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13\n",
      "Train loss 0.14749037082258024 train_acc 0.9846491228070174\n",
      "Valid loss 0.11494898051023483 valid_acc 0.99375\n",
      "Test loss 0.4608079567551613 test acc 0.8849158653846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 275.71it/s]\n",
      "5it [00:00, 830.72it/s]\n",
      "8it [00:00, 666.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\n",
      "Train loss 0.12919698420323825 train_acc 0.9868421052631579\n",
      "Valid loss 0.10325817316770554 valid_acc 0.990909090909091\n",
      "Test loss 0.451061999425292 test acc 0.8888221153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 301.59it/s]\n",
      "5it [00:00, 833.39it/s]\n",
      "8it [00:00, 666.29it/s]\n",
      " 75%|███████▌  | 15/20 [00:01<00:00, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\n",
      "Train loss 0.11675233315480382 train_acc 0.9868421052631579\n",
      "Valid loss 0.08611958250403404 valid_acc 0.99375\n",
      "Test loss 0.45040670968592167 test acc 0.8966346153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 294.92it/s]\n",
      "5it [00:00, 833.39it/s]\n",
      "8it [00:00, 1000.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16\n",
      "Train loss 0.10422442284853835 train_acc 0.9901315789473685\n",
      "Valid loss 0.08201652914285659 valid_acc 1.0\n",
      "Test loss 0.47118449583649635 test acc 0.8852163461538461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 289.98it/s]\n",
      "5it [00:00, 999.98it/s]\n",
      "8it [00:00, 799.89it/s]\n",
      " 85%|████████▌ | 17/20 [00:01<00:00, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17\n",
      "Train loss 0.09066050676138777 train_acc 0.9945175438596491\n",
      "Valid loss 0.07204568386077881 valid_acc 1.0\n",
      "Test loss 0.4426756836473942 test acc 0.8975360576923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 281.29it/s]\n",
      "5it [00:00, 623.69it/s]\n",
      "8it [00:00, 1000.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18\n",
      "Train loss 0.08182467186921522 train_acc 0.9983552631578947\n",
      "Valid loss 0.06482832953333854 valid_acc 1.0\n",
      "Test loss 0.4498145915567875 test acc 0.8957331730769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 233.04it/s]\n",
      "5it [00:00, 554.49it/s]\n",
      "8it [00:00, 999.80it/s]\n",
      " 95%|█████████▌| 19/20 [00:01<00:00, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19\n",
      "Train loss 0.07475087252494536 train_acc 0.9983552631578947\n",
      "Valid loss 0.05722342878580093 valid_acc 1.0\n",
      "Test loss 0.4502667561173439 test acc 0.9014423076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 275.19it/s]\n",
      "5it [00:00, 833.29it/s]\n",
      "8it [00:00, 1000.40it/s]\n",
      "100%|██████████| 20/20 [00:02<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20\n",
      "Train loss 0.06720493439781039 train_acc 1.0\n",
      "Valid loss 0.05300777927041054 valid_acc 1.0\n",
      "Test loss 0.4521252065896988 test acc 0.8987379807692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 开始第一次debug\n",
    "train(model,optimizer,loss_function,epoch,TrainLoader,ValidLoader,TestLoader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random  # to set the python random seed\n",
    "import numpy  # to set the numpy random seed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# Ignore excessive warnings\n",
    "import logging\n",
    "\n",
    "logging.propagate = False\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# WandB – Import the wandb library\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torchvision.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchvision.datasets.mnist' from 'd:\\\\anaconda3\\\\envs\\\\pytorch\\\\lib\\\\site-packages\\\\torchvision\\\\datasets\\\\mnist.py'>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: zhijiao. Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个简单的卷积神经网络\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 卷积 -> 池化 -> ReLU\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        \n",
    "        # flatten\n",
    "        \n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "\n",
    "        # fc1 -> ReLU -> fc2 -> ReLU -> fc3\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # softmax\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, device, train_loader, optimizer, epoch):\n",
    "    # switch model to training mode. This is necessary for layers like dropout, batchNorm etc.\n",
    "    # which behave differently in training and evaluation mode.\n",
    "    model.train()\n",
    "\n",
    "    # we loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "        if batch_id > 20:\n",
    "            break\n",
    "        # Loop the input features and labels from the training dataset.\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Reset the gradients to 0 for all learnable weight parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Pass image data from training dataset, make predictions\n",
    "        # about class image belongs to (0-9 in this case).\n",
    "        output = model(data)\n",
    "\n",
    "        # Define our loss function, and compute the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Backward pass:compute the gradients of loss,the model's parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update the neural network weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log用来记录一些日志(accuracy,loss and epoch), 便于随时查看网路的性能\n",
    "def test(args, model, device, test_loader, classes):\n",
    "    model.eval()\n",
    "    # switch model to evaluation mode.\n",
    "    # This is necessary for layers like dropout, batchNorm etc. which behave differently in training and evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    example_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Load the input features and labels from the test dataset\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Make predictions: Pass image data from test dataset,\n",
    "            # make predictions about class image belongs to(0-9 in this case)\n",
    "            output = model(data)\n",
    "\n",
    "            # Compute the loss sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "\n",
    "            # Get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # Log images in your test dataset automatically,\n",
    "            # along with predicted and true labels by passing pytorch tensors with image data into wandb.\n",
    "            example_images.append(wandb.Image(\n",
    "                data[0], caption=\"Pred:{} Truth:{}\".format(classes[pred[0].item()], classes[target[0]])))\n",
    "\n",
    "   # wandb.log(a_dict) logs the keys and values of the dictionary passed in and associates the values with a step.\n",
    "   # You can log anything by passing it to wandb.log(),\n",
    "   # including histograms, custom matplotlib objects, images, video, text, tables, html, pointclounds and other 3D objects.\n",
    "   # Here we use it to log test accuracy, loss and some test images (along with their true and predicted labels).\n",
    "    wandb.log({\n",
    "        \"Examples\": example_images,\n",
    "        \"Test Accuracy\": 100. * correct / len(test_loader.dataset),\n",
    "        \"Test Loss\": test_loss\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71342f419acc3ba6ae382518c4ba2a9e6f9bd8751a76a463bc8e77674675b221"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
